# Chicago Environmental Storage Tanks Analysis

A comprehensive big data analytics project analyzing environmental storage tank data from Chicago. This project demonstrates the complete big data ecosystem workflow from data ingestion to visualization, utilizing Hadoop MapReduce, Apache Spark, HBase, and interactive dashboards.

## üìä Project Overview

This project analyzes Chicago's environmental storage tank dataset, tracking installations, removals, tank types, capacities, products stored, and geographical distribution across the city. The analysis provides insights into:

- Historical trends of tank installations and removals
- Tank capacity distributions and product types
- Geographic distribution across Chicago suburbs
- Environmental compliance and safety patterns
- Long-term infrastructure trends

## üõ†Ô∏è Technology Stack

### Big Data Processing
- **Apache Hadoop** (v3.3.6) - Distributed storage and MapReduce processing
- **Apache Spark** (v3.4.1) - Fast in-memory analytics and data processing
- **Apache HBase** (v2.5.8) - NoSQL distributed database for tank data storage

### Programming Languages
- **Java 8** - Primary language for MapReduce and Spark jobs
- **Python 3** - Data visualization and dashboard development

### Data Visualization
- **Plotly** - Interactive charts and graphs
- **Dash** - Web-based analytics dashboard
- **Dash Bootstrap Components** - UI framework
- **ydata-profiling** - Automated data profiling reports

### Build & Dependency Management
- **Apache Maven** - Project build and dependency management
- **Maven Shade Plugin** - Creating uber-JARs for cluster deployment

### Data Format
- **CSV** - Input data format
- **HDFS** - Distributed file system storage

## üìÅ Project Structure

The project is organized across multiple branches, each representing a different phase of the big data pipeline:

### Branch: `master` - Hadoop MapReduce Jobs
**Purpose**: Initial data processing using Hadoop MapReduce framework

**Key Components**:
- `TanksByProduct.java` - Counts storage tanks by product type
- `TankCapacityAnalysis.java` - Analyzes tank capacity distributions
- `InstallationsPerYear.java` - Tracks installation trends over time
- `RemovalsPerYear.java` - Tracks removal trends over time
- `TankTypeDistribution.java` - Analyzes distribution of tank types (underground vs. aboveground)
- `InstallationsPerSuburb.java` - Geographic distribution analysis

**Achievements**:
- ‚úÖ Implemented 6+ MapReduce jobs for comprehensive data analysis
- ‚úÖ Processed CSV data with proper field parsing
- ‚úÖ Generated aggregated outputs for downstream analysis

### Branch: `Hbase` - NoSQL Data Storage
**Purpose**: Load structured tank data into HBase for fast random access

**Key Components**:
- `HelloHBase.java` - Data ingestion pipeline from CSV to HBase

**Data Schema**:
- **Row Key**: `facilityId#tankId` (composite key)
- **Column Families**:
  - `info` - Tank metadata (owner, type, material, construction)
  - `location` - Geographic data (address, suburb)
  - `status` - Temporal data (installation date, removal date)
  - `product` - Product details (tank product, capacity)

**Achievements**:
- ‚úÖ Designed efficient HBase schema with logical column families
- ‚úÖ Implemented CSV-to-HBase ETL pipeline
- ‚úÖ Configured ZooKeeper integration for distributed coordination

### Branch: `Spark` - Advanced Analytics
**Purpose**: High-performance data analytics using Apache Spark

**Key Spark Jobs**:
1. **NetGrowthJob.java** - Calculates net growth (installations - removals) by year
2. **TopProductsJob.java** - Identifies most common products stored in tanks
3. **TrendJob.java** - Computes 5-year rolling average of installations
4. **CapacityBucketsJob.java** - Categorizes tanks by capacity ranges
5. **Additional analyses** - Building on MapReduce jobs with Spark optimization

**Technical Highlights**:
- Spark SQL for declarative data transformations
- DataFrame API for efficient processing
- Date parsing and temporal analysis
- Aggregation and windowing functions

**Achievements**:
- ‚úÖ Migrated MapReduce jobs to Spark for 10-100x performance improvement
- ‚úÖ Implemented advanced analytics (rolling averages, bucketing)
- ‚úÖ Generated cleaned datasets for visualization

### Branch: `spark-output` - Processed Data Outputs
**Purpose**: Stores merged and cleaned outputs from Spark jobs

**Output Files**:
- `net_merged.csv` - Year-by-year installations, removals, and net growth
- `top_products.csv` - Top products by tank count
- `capacity_merged.csv` - Tank distribution by capacity buckets
- `pivot_merged.csv` - Tank type trends (underground vs. aboveground) over time
- `trend.csv` - 5-year moving average of installations
- `suburb.csv` - Geographic distribution across suburbs

**Achievements**:
- ‚úÖ Consolidated 100+ Spark output partitions into single CSV files
- ‚úÖ Clean, ready-to-visualize datasets
- ‚úÖ Standardized schema across all outputs

### Branch: `viz` - Interactive Dashboard
**Purpose**: Web-based interactive dashboard for data exploration

**Dashboard Features**:
1. **Capacity Distribution** - Bar chart showing tank counts by capacity range
2. **Net Growth Analysis** - Line chart comparing installations vs. removals over time
3. **Tank Type Trends** - Area chart showing evolution of underground vs. aboveground tanks
4. **Top Products** - Horizontal bar chart of most common stored products
5. **Suburb Distribution** - Geographic analysis of tank distribution
6. **5-Year Rolling Trend** - Smoothed installation trend analysis
7. **Automated Data Profiling** - ydata-profiling reports for each dataset

**Technical Stack**:
- Dash framework with Bootstrap styling
- Plotly for interactive visualizations
- Tabbed interface for organized navigation
- Embedded profiling reports for data quality insights

**Achievements**:
- ‚úÖ Built fully interactive web dashboard
- ‚úÖ Integrated 6 different visualization types
- ‚úÖ Automated data profiling for quality assurance
- ‚úÖ Responsive design with Bootstrap components

## üöÄ Getting Started

### Prerequisites
- Java Development Kit (JDK) 8 or higher
- Apache Maven 3.6+
- Apache Hadoop 3.3.6 (for MapReduce jobs)
- Apache Spark 3.4.1 (for Spark jobs)
- Apache HBase 2.5.8 (for database operations)
- Python 3.8+ (for visualization dashboard)

### Building the Project

#### MapReduce Jobs (master branch)
```bash
git checkout master
mvn clean package
# Run example job
hadoop jar target/chicago_env-1.0-SNAPSHOT.jar bigdataproject.jobs.TanksByProduct input/file.csv output/products
```

#### Spark Jobs (Spark branch)
```bash
git checkout Spark
mvn clean package
# Run example Spark job
spark-submit --class bigdataproject.jobs.NetGrowthJob target/chicago_env-1.0-SNAPSHOT.jar
```

#### HBase Data Loading (Hbase branch)
```bash
git checkout Hbase
# Ensure HBase and ZooKeeper are running
java -cp target/chicago_env-1.0-SNAPSHOT.jar tn.insat.tp4.HelloHBase
```

#### Visualization Dashboard (viz branch)
```bash
git checkout viz
pip install -r requirements.txt
python app.py
# Access dashboard at http://localhost:8050
```

## üìà Key Insights from Analysis

Based on the processed data outputs:

1. **Product Distribution**:
   - Gasoline: 5,546 tanks (largest category)
   - Heating Oil: 3,678 tanks
   - Diesel: 2,182 tanks
   - Fuel Oil: 860 tanks

2. **Historical Trends**:
   - Data spans from 1901 to present
   - Significant installation activity in mid-20th century
   - Recent trends show declining installations

3. **Tank Types**:
   - Predominance of underground storage tanks
   - Shift in tank technology over decades
   - Safety and environmental compliance evolution

## üîÑ Data Pipeline Workflow

```
Raw CSV Data
    ‚Üì
[Hadoop MapReduce] ‚Üí Initial aggregations and filtering
    ‚Üì
[Apache HBase] ‚Üí Structured NoSQL storage with fast access
    ‚Üì
[Apache Spark] ‚Üí Advanced analytics and transformations
    ‚Üì
[Processed CSVs] ‚Üí Clean, merged datasets
    ‚Üì
[Dash Dashboard] ‚Üí Interactive visualizations and insights
```

## üìä Dataset Information

**Source**: Chicago Environmental Storage Tanks dataset

**Fields**:
- Address information (street, number, direction)
- Tank specifications (type, material, construction)
- Product and capacity details
- Temporal data (installation, removal, last used dates)
- Geographic coordinates (latitude, longitude)
- Facility and owner information
- Regulatory and compliance data

## ü§ù Contributing

This project demonstrates a complete big data analytics workflow. Contributions are welcome for:
- Additional analytics jobs
- Enhanced visualizations
- Performance optimizations
- Documentation improvements

## üìù License

This project is for educational and analytical purposes, demonstrating big data technologies and workflows.

## üéØ Learning Outcomes

This project demonstrates proficiency in:
- ‚úÖ Distributed computing with Hadoop MapReduce
- ‚úÖ Real-time analytics with Apache Spark
- ‚úÖ NoSQL database design with HBase
- ‚úÖ ETL pipeline development
- ‚úÖ Data visualization and dashboard creation
- ‚úÖ Maven project management
- ‚úÖ Big data ecosystem integration
- ‚úÖ Java and Python development for data engineering

---

**Project Status**: Complete and functional across all branches

**Last Updated**: February 2026